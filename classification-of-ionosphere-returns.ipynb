{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<figure>\n  <center>\n      <img src=\"https://i.imgur.com/Keqba8s.png\" alt=\"Trulli\" style=\"width:80%; height:50%\">\n  </center>\n</figure>\n\n# 1. Introduction\n<font size=\"4.5\">\nThe ionosphere is the ionized part of Earth's upper atmosphere, from about 48 km 965 km altitude. Our task is to distinguish different type of radar returns to understand if at the time of return ionosphere had some meaningful structure of free electrons. We perform a comprehensive analysis of data at hand and use well known Machine learning algorithms to perform this fully supervised classification task.  \n</font>\n\n# 2. Dataset Description\n<font size=\"4.5\">\nThis radar data was collected by a system in Goose Bay, Labrador. This system consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts.  \nThe targets were free electrons in the ionosphere. \"Good\" radar returns are those showing evidence of some type of structure in the ionosphere. \"Bad\" returns are those that do not; their signals pass through the ionosphere.  \n<br>\nExtreme UltraViolet (EUV) and x-ray solar radiation ionizes the atoms and molecules inside the Ionosphere thus creating a layer of electrons. The ionosphere is important because it reflects and modifies radio waves used for communication and navigation. Other phenomena such as energetic charged particles and cosmic rays also have an ionizing effect and can contribute to the ionosphere. \n<br>\n<center>\n    <img src=\"https://i.imgur.com/DUeHs86.png\" alt=\"Drawing\" style=\"width: 700px;height: 850px\" class=\"center\"/>\n</center>\n<br>\nThe radar operates by transmitting a multipulse pattern to the ionosphere. It produces 17 pairs of numbers every 0.2 s all year round. Received signals were processed using an autocorrelation function whose arguments are the time of a pulse and the pulse number. There were 17 pulse numbers for the Goose Bay system. Instances in this dataset are described by 2 attributes per pulse number, corresponding to the complex values returned by the function resulting from the complex electromagnetic signal. \n</font>","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom pprint import pprint\n\npd.set_option(\"display.precision\", 5)\nwarnings.filterwarnings(\"ignore\")\nsns.set(rc={\"figure.dpi\":100, 'savefig.dpi':300})\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-07-18T00:44:54.244417Z","iopub.execute_input":"2021-07-18T00:44:54.244827Z","iopub.status.idle":"2021-07-18T00:44:54.254472Z","shell.execute_reply.started":"2021-07-18T00:44:54.244793Z","shell.execute_reply":"2021-07-18T00:44:54.253552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rename(x):\n    if 'feature' in x:\n        return x.replace(\"feature\", \"f\")\n    return x\n\nfile = \"/kaggle/input/uci-ionosphere/ionosphere_data_kaggle.csv\"\n\ndf = pd.read_csv(file)\ntarget_map = {\"g\" : 1, \"b\" : 0}\ndf[\"label\"] = df.label.map(target_map).astype(np.int)\ndf = df.rename(columns= rename)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T00:44:54.29286Z","iopub.execute_input":"2021-07-18T00:44:54.29347Z","iopub.status.idle":"2021-07-18T00:44:54.315522Z","shell.execute_reply.started":"2021-07-18T00:44:54.293432Z","shell.execute_reply":"2021-07-18T00:44:54.314633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 Exploratory Data Analysis\n## 3.1 Attribute information\n<font size=\"4.5\">\nThe dataset in analysis contains 351 distinct radar returns. Each return produces 17 pairs of complex numbers. Numbers are reported as features and divided into real and imaginary parts giving us a total of 34 features.  \nAll features are continuous and contain no missing values. \"Good\" returns are labelled as 1 whereas \"Bad\" returns are labelled as 0.\n</font>\n  \n### First five instances of the dataset:\n","metadata":{}},{"cell_type":"code","source":"df.head().T","metadata":{"execution":{"iopub.status.busy":"2021-07-18T00:44:54.39351Z","iopub.execute_input":"2021-07-18T00:44:54.39411Z","iopub.status.idle":"2021-07-18T00:44:54.4202Z","shell.execute_reply.started":"2021-07-18T00:44:54.394074Z","shell.execute_reply":"2021-07-18T00:44:54.418883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Statistical overview:  \n<font size=\"4.5\">\nFor each feature, we briefly analyze their statistical properties such as mean, standard deviation, minimum, maximum and quartiles. It helps us understand the scale of different features which plays a key role in several machine learning models using distance based approaches.\n</font>","metadata":{}},{"cell_type":"code","source":"df.drop(\"label\", axis = 1).describe().T","metadata":{"execution":{"iopub.status.busy":"2021-07-18T00:44:54.434088Z","iopub.execute_input":"2021-07-18T00:44:54.434664Z","iopub.status.idle":"2021-07-18T00:44:54.540336Z","shell.execute_reply.started":"2021-07-18T00:44:54.434602Z","shell.execute_reply":"2021-07-18T00:44:54.539144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data distribution\n<font size=\"4.5\">\nSince all features are numerical attributes, we plot their distribution with histograms and estimate their kernel density.\n</font>","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"whitegrid\")\n\nnum_cols = df.select_dtypes(include= ['Float64', 'int64']).columns\nsns.set(font_scale=1.5)\nfig, axes = plt.subplots(nrows=9, ncols=4, figsize=(20,24))\nfig.delaxes(axes[8,2])\nfig.delaxes(axes[8,3])\n\nfig.subplots_adjust(hspace=0.5)\nfig.suptitle('Distributions of numerical Features', fontsize = 36)\n\n\nfor ax, feats in zip(axes.flatten(), num_cols):\n    fig = sns.distplot(a=df[feats], ax=ax, kde_kws={\"lw\":2}, color=\"k\")\n\nplt.figtext(0.5,-0.025,\"Figure 1: Distributions of dataset attributes.\", fontsize = 22, ha = 'center')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T00:44:54.542266Z","iopub.execute_input":"2021-07-18T00:44:54.542727Z","iopub.status.idle":"2021-07-18T00:45:01.239839Z","shell.execute_reply.started":"2021-07-18T00:44:54.542679Z","shell.execute_reply":"2021-07-18T00:45:01.238786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Target class distribution\n<font size=\"4.5\">\nSince many algorithms are sensitive to unbalanced data, the distribution of the classes \nis plotted. The dataset is not 'perfectly' balanced as we have one 'majority' class with approximately 64% of instances and one 'minority' class with 36% of total instances.\n</font>","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nplt.figure(figsize = (5,8))\nax = sns.countplot(x='label',data=df, dodge = False)\n\nbars = ax.patches\nhalf = int(len(bars)/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\n\nfor left, right in zip(left_bars, right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    total = height_l + height_r\n\n    ax.text(left.get_x() + left.get_width()/2., \n            height_l -20, '{0:.2%}'.format(height_l/total), ha=\"center\", fontsize = 18)\n    ax.text(right.get_x() + right.get_width()/2., \n            height_r -20, '{0:.2%}'.format(height_r/total), ha=\"center\", fontsize = 18)\n\n\nplt.xlabel(\"label\")\nplt.ylabel(\"Counts\")\nplt.title(\"Distribution of target class\", fontsize = 20)\nplt.figtext(0.5,-0.025,\"Figure 2: Number of samples for each class\", fontsize = 16, ha = 'center')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T00:45:01.241364Z","iopub.execute_input":"2021-07-18T00:45:01.241662Z","iopub.status.idle":"2021-07-18T00:45:01.407776Z","shell.execute_reply.started":"2021-07-18T00:45:01.241634Z","shell.execute_reply":"2021-07-18T00:45:01.406728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Feature Correlation\n<font size=\"4.5\">\nA very useful way to analyze the correlation between numerical features is the correlation matrix.  \nFor each pair of attributes we can compute the covariance and divide it by the product of the standard deviations.\n</font>\n\n$$\nCov(x,y) = \\frac{1}{n-1} \\sum \\limits _{i = 1} ^{n} (x_i - \\mu_x)(y - \\mu_y)\n$$  \n\n$$\n\\rho = \\frac{Cov(x,y)}{S_x  S_y}\n$$\n<font size=\"4.5\">\nWhere $\\mu_x$ and $\\mu_y$ are sample means of arbitrary x, y features and $S_x$, $S_y$ are their respective standard deviations.  $\\rho$ is called the Pearson correlation coefficient and it measures the linear correlation between two sets of data.  $\\rho$ is a continuous number in between -1 and 1.  If two random variables are statistically independent, corresponding correlation values are 0. It is important to notice that the opposite is not true, two random variables may show 0 correlation and may yet be completely dependent on each other. This is due to the fact that $\\rho$ measures only \"linear\" correlation and datasets may have non linear dependencies. \n\n<figure>\n  <center>\n  <img src=\"https://i.imgur.com/g7L9k6f.png\" alt=\"Drawing\" style=\"width: 800px;height: 300px\"/>\n    </center>\n</figure>\n</font>","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"white\")\ncorr_df = df.corr()\nmask = np.triu(corr_df)\nmask = mask[1:, :-1]\ncorr_df = corr_df.iloc[1:,:-1].copy()\n\nf,ax=plt.subplots(figsize=(24,20))\nfig = sns.heatmap(corr_df,annot=True,fmt=\".2f\", ax=ax, \n                  linecolor=\"white\", linewidths=0.1 ,\n                  mask = mask, square=True, \n                  annot_kws={\"size\": 9}, cbar_kws = {\"shrink\":0.5, 'label': 'Correlation'},\n                  cmap= 'coolwarm', rasterized=False)\n\nplt.xticks(rotation= 90, ha = 'center')\nplt.yticks(rotation= 0)\n\nplt.title('Pearson Correlation Heatmap of the original data set', fontdict= {\"fontsize\":32})\nplt.figtext(0.5,.1, \"Figure 4: Correlation map\", fontsize = 20, ha = 'center')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T00:45:01.409209Z","iopub.execute_input":"2021-07-18T00:45:01.409497Z","iopub.status.idle":"2021-07-18T00:45:04.923602Z","shell.execute_reply.started":"2021-07-18T00:45:01.40947Z","shell.execute_reply":"2021-07-18T00:45:04.922369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\n\n- Feature 2 contations only 0 values and it encodes no useful information\n- Feature 13 and 15 are strongly correlated with each other \n- Features 1,3,5,7 show a mild correlation with target label whereas features 20,24,26,28,30,32 and 34 show near to 0 correlation with target label.\n    ","metadata":{}},{"cell_type":"markdown","source":"# 4 Constructing the pipeline\n\n## 4.1 Partitioning\nTo better understand the performance of a trained model, it is essential that we test it on a subset of data which is yet-unseen by our model. Otherwise we might overfit our model to provide excellent performance on available dataset but when we feed new, yet-unseen, data samples into our machine learning model it provides nonsense results. Overfitting is a situation in which our model has very low bias and high variance of predictors.  \nTo replicate the effect of testing on yet-unseen data, we split data at hand into training and test sets. 25% of the total amount of data is allocated for testing purposes. This subset of data is used _only_ to test the performance of the final model __at the end__. To maintain the IID assumption about dataset instances, we used stratified sampling technique in which the distribution of each target label is kept the same for both training and test datasets.  \n\n\nTo find appropriate hyperparameters for a classification model, we need another set of data which we call the validation dataset.  We choose Stratified k-fold cross validation technique. It is a validation technique for assessing how the results of a statistical analysis will generalize to an independent data set. It works as follows:  \n\n1. We split our data into _K_ equal sized sets\n2. We use _K-1_ subsets to train the model and _K_'th subset is used to evaluate its performance\n3. We repeat step-2 _K_ times by replacing each time the validation set\n4. The _K_ results can then be averaged to produce a single estimation and we select the hyperparameters which provide best estimation  \n\n\n<figure>\n  <center>\n  <img src=\"https://i.imgur.com/sF2b5mm.png\" alt=\"Drawing\" style=\"width: 650px;height: 450px\"/>\n    </center>\n</figure>\n\nThe advantage of this method is that all observations are used for both training and validation, and each observation is used for validation exactly once.  Stratified means that each subset contains the same underlying distribution of target labels. In case of binary classification it means that each partition set contains approximately the same proportion of positive and negative class labels. Using stratified splits is important as we want to evaluate our models performance on the dataset which has the same data distribution as our test data set. \n\n\n## 4.2 Features selection\nIn our work we approach dimensionality reduction (See section) from two different angles.  \nFirst we use a manual features selection by analyzing their correlation with other features and target labels. We drop (one of those) features which have $|\\rho| > .80$ or the ones which has $|\\rho| < 0.05 $ with target variable.  \nSince $\\rho$ only indicates the linear relationship among predictors, such selection can also result in loss of useful information. To avoid such cases we empirically tested our machine learning models with and without these predictors and we got comparable results. This feature selection method allows us to directly interpret the results of machine learning models as training data stays in its original space.  \nWe introduce feature projection techniques in the second part of our work. These approaches use linear projections of original data in such a way that newly created features space encodes certain data properties and has much lower dimensionality.  \n\n## 4.3 Metrics\n<font size=\"4.5\">\nThe metrics used to classify are the following.\n</font>\n\n### Accuracy\nAccuracy is the proportion of true results among the total number of cases examined:\n \n$$\nAccuracy = \\frac{TP + FN}{TP + FP + FN + TN}\n$$\nAccuracy is probably the most famous and used performance metric but it does not tell the whole story all the time.  \nIn the case of unbalanced data, for example 99% vs 1%, a naive model can achieve a 99% accuracy score by predicting everything as the majority class.  \n\n### F1 Score\n    \nF1 score has two main ingredients:\n* __Precision__ for a class is the number of true positives (i.e. the number of items correctly labelled as belonging to the positive class) divided by the total number of elements labelled as belonging to the positive class:\n$$\n    Precision = \\frac{TP}{TP+ FP}\n$$\n \n* __Recall__ (also known as _sensitivity_) is defined as the number of true positives divided by the total number of elements that actually belong to the positive class\n$$\n    Recall = \\frac{TP}{TP+ FN}\n$$\n \nThe F1 score is a number between 0 and 1 and is the harmonic mean of precision and recall.\n \n$$\n    F_1 = 2 * \\frac{Precision * Recall}{Precision + Recall}\n$$\n \nWe select F1-score as a performance metric and K-fold cross validation measures the average F1-score for each fold to find the best set of hyperparameters.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\n\ndrop_label = [\"label\"]\ndrop_lowCorr = [f\"f{i}\" for i in [2,13,20,24,26,28,30,32,34]]\n\ny, X = df[\"label\"], df.drop(drop_label, axis=1)\n\nrs = 99\nts = 0.25\nX_train_all, X_test_all, y_train, y_test = train_test_split(X, y, test_size= ts, \n                                                    stratify= y, random_state=rs)\n\nX_train, X_test = X_train_all.drop(drop_lowCorr, axis=1), X_test_all.drop(drop_lowCorr, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T00:45:04.925271Z","iopub.execute_input":"2021-07-18T00:45:04.925766Z","iopub.status.idle":"2021-07-18T00:45:04.9402Z","shell.execute_reply.started":"2021-07-18T00:45:04.925717Z","shell.execute_reply":"2021-07-18T00:45:04.938968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.4 Feature scaling\nAlthough all features of our dataset are continuous values between [-1, 1] range, we prefer converting them into standard normal distribution using a well known standardization process. It works as follows,\n\n$$\nz = \\frac{x - \\mu}{\\sigma}\n$$\nwhere\n- x is the original feature vector\n- $\\mu$ is the sample mean for that feature\n- $\\sigma$ is the standard deviation of the x feature  \n\nTo avoid any kind of data leakage $\\mu$ and $\\sigma$ are calculated only from training data and the transform is applied to both training and test sets. \n\n## 4.4 Outliers management\n\nIn statistics, an outlier is an observation point that is distant from other observations.  \nDifferent machine learning methods behave differently when they are trained on data which has outliers. Hard margin SVM is an example of a machine learning model which is highly sensitive to outliers.  \n\nTo detect outliers within data at hand, we observe the distribution of scaled training data set by using boxplots. A boxplot nicely encodes the data distribution using interquartile range. (Figure 7)\n\n<!-- ![alt text](bp.png) -->\n\n<figure>\n  <center>\n      <img src=\"https://i.imgur.com/IDxKaYz.png\" alt=\"Drawing\" style=\"width: 550px;height: 300px\"/>\n  </center>\n</figure>\nThe figure 8 shows the distribution of the 'Good' and 'Bad' return samples after the normalization process, labeling as outliers all the points lower than \"Q1 âˆ’1.5IQR\" or higher than \"Q3 +1.5IQR\". Where Q1 and Q3 are, respectively, the lower and upper quartile and IQR is the interquartile range. (Figure 7)\n","metadata":{}},{"cell_type":"code","source":"mean_train, std_train = X_train.mean(0), X_train.std(0)\nX_train_std = (X_train - mean_train) / std_train\n\nfig, axs = plt.subplots(2,1,figsize = (20,12), sharex= True)\nsns.boxplot(data = X_train_std[y_train == 1], palette=\"pastel\", linewidth=2, ax = axs[0], whis= 1.5)\n# plt.figure(figsize = (16,6))\nsns.boxplot(data = X_train_std[y_train == 0], palette=\"pastel\", linewidth=2, ax = axs[1], whis= 1.5)\n# plt.title(' Correlation Heatmap of the original data set', fontdict= {\"fontsize\":32})\nplt.figtext(0.5,-0.05, \"Figure 8: Boxplots of training data w.r.t target labels. \\\nGood radar returns (top), Bad radar returns (bottom)\", fontsize = 18, ha = 'center')\nplt.xlabel(\"features\", size = 16)\nfor ax in axs:\n    ax.grid()\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T00:45:04.941755Z","iopub.execute_input":"2021-07-18T00:45:04.942053Z","iopub.status.idle":"2021-07-18T00:45:06.666404Z","shell.execute_reply.started":"2021-07-18T00:45:04.942018Z","shell.execute_reply":"2021-07-18T00:45:06.665173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we remove all the outliers using interquartile rule, it would impact mostly the _'Good'_ radar returns. _'Bad'_ returns have very few outliers and most of them are within the range of 3$\\sigma$ (standard deviation) from mean. 3$\\sigma$ corresponds to 99.7% of the area under the bell curve of Standard normal distribution.  \nRemoving outliers will definitely change the underline distribution of _'Good'_ and _'Bad'_ class labels. We may distinguish _'Good'_ returns from bad ones considering them as _outsiders_, hence removing outliers from _'Good'_ returns is not preferable.\n","metadata":{}},{"cell_type":"markdown","source":"## 4.5 Class Balancing \n\nClass imbalance is a phenomenon in which one target label is highly prevalent in a given dataset. Datasets like fraud or disease (e.g. Cancer) detection often suffer from this issue as one class occurs rarely in training data. It makes it harder for the learner to make reliable predictions when working imbalanced data.  \nAlthough the dataset in our case is not highly imbalanced, it is neither perfectly balanced. To make the distribution of target classes even, we use an oversampling technique called SMOTE.  \nSynthetic Minority Over-sampling Technique or simply SMOTE was first introduced by Chawla, et al. in 2012.  It selects a random data point $x_i$ from the minority class and finds its k-nearest minority class neighbors where k is a hyperparameter. Among those k neighbors, SMOTE selects at random one point $x_{zi}$ and a new sample is created on the segment joining $x_i$ with $x_{zi}$. The new point is defined as $x_{new} = x_i + \\lambda(x_i - x_{zi})$ s.t $\\lambda \\in [0,1]$\n\n\n<figure>\n  <center>\n      <img src=\"https://i.imgur.com/Sseq10Q.png\" alt=\"Trulli\" style=\"width: 1200px;height: 700px\"/>\n  </center>\n</figure>\n\n\nAny methods which alters the underlying distribution of our data, must only be applied on training data. Therefore SMOTE is applied only on the training data and test\\validation datasets are unchanged.\nThe full classification pipeline is shown in figure __.\n\n<figure>\n  <center>\n      <img src=\"https://i.imgur.com/VWv8Lqn.png\" alt=\"Trulli\" style=\"width:50%; height:50%\">\n  </center>\n</figure>\n","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n# from sklearn.pipeline import make_pipeline, Pipeline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import make_pipeline\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import tree\n\nfrom sklearn.metrics import classification_report, f1_score, auc, accuracy_score\nfrom sklearn.metrics import confusion_matrix,roc_auc_score,roc_curve","metadata":{"execution":{"iopub.status.busy":"2021-07-18T00:45:06.670055Z","iopub.execute_input":"2021-07-18T00:45:06.670597Z","iopub.status.idle":"2021-07-18T00:45:06.678371Z","shell.execute_reply.started":"2021-07-18T00:45:06.670523Z","shell.execute_reply":"2021-07-18T00:45:06.677157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\ndef dprint(d):\n    print(json.dumps(d, sort_keys=False, indent=4),\"\\n\")\n    \n    \ndef report(y_true, y_pred, title = \"\", \n           params = None, show_cm = True, show_params = True):\n\n    nl = '\\n'\n    sp = \"*\"*len(title)\n    \n#     print(\"__\"*30 + nl)\n    if title:\n        title = \"Classification report for \" + title + \" on yet-unseen data\"\n        sp = \"*\"*len(title)\n        print(sp + nl + title + nl + sp +nl)\n    \n    acc = accuracy_score(y_true, y_pred)\n    f_score = f1_score(y_true, y_pred)\n    print(\"Accuracy: {:.3f}\".format(acc))\n    print(\"f-score: {:.3f}\".format(f_score))\n    \n    if params and show_params:\n        new_params = {}\n        for k in params:\n            new_params[k.split(\"__\")[-1]] = params[k]\n            \n        print(\"\\nBest Parameters:\")\n#         pprint(new_params, indent = 4)\n        dprint(new_params)\n        \n#     print(classification_report(y_true, y_pred))\n    if show_cm:\n        plt.figure(figsize = (6,5))\n        cm = confusion_matrix(y_true, y_pred)\n        sns.heatmap(cm, annot = True, \n                    cmap = 'YlGnBu', \n                    annot_kws={\"size\": 18},\n                    linecolor = 'w',\n                    linewidth = 4\n                   )\n        plt.title(\"Confusion Matrix\")\n        plt.xlabel(\"Predicted labels\")\n        plt.ylabel(\"True labels\")\n    \n    return (acc, f_score)\n    \nscores = {}\ngs_trained_models =  {}","metadata":{"execution":{"iopub.status.busy":"2021-07-18T01:06:00.175586Z","iopub.execute_input":"2021-07-18T01:06:00.176183Z","iopub.status.idle":"2021-07-18T01:06:00.187677Z","shell.execute_reply.started":"2021-07-18T01:06:00.176146Z","shell.execute_reply":"2021-07-18T01:06:00.186742Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def run_tests(model, X_train, X_test, y_train, y_test, \n              params, scores_dict, model_dict, show_cm = True, show_params = True):\n    grid_search = GridSearchCV(model, params, \n                               scoring = 'f1',\n                               n_jobs=5,\n                               cv=5,\n                    )\n    grid_search.fit(X_train, y_train)\n    y_pred = grid_search.predict(X_test)\n    model_dict[name] = grid_search.best_estimator_\n    acc, fscore = report(y_test, y_pred, name, grid_search.best_params_, show_cm, show_params)\n    scores_dict[name] = fscore\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T00:45:06.699668Z","iopub.execute_input":"2021-07-18T00:45:06.700154Z","iopub.status.idle":"2021-07-18T00:45:06.7176Z","shell.execute_reply.started":"2021-07-18T00:45:06.700101Z","shell.execute_reply":"2021-07-18T00:45:06.71647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Analysis of classification models\n## 5.1 Decision Trees\n\nDecision Tree is a supervised learning algorithm whose goal is to create a training model to predict the class or value of the target variable by learning simple decision rules inferred from training data. Decision trees are locally constant functions which divide predictor space into non-overlapping regions having boundaries parallel to the axis. The label assigned is the most commonly occurring class in that region.  \nAt each step the model chooses the best predictor to split according to some criteria. Since it is computationally infeasible to consider every possible partition of the feature space when training the model, we take a top-down greedy approach that is known as recursive binary splitting. This approach is greedy because at each step we perform a locally optimal split, rather than the one that would lead to the best global result.  \nGini index and cross entropy are two well known criteria to measure the goodness of a split.  \n\n__1. GINI index:__  \n    GINI index is an impurity measure of a given node and it is lowest when a node contains only samples belonging to one single class. It is defined as.\n    $$\n        GINI = 1 - \\sum_{k = 1}^{K} {P}_{mk} log({P}_{mk})\n    $$\n    ${P}_{mk}$ is the proportion of k'th class in region m and K is the total number of classes.\n    We prefer the split which yields the least GINI index.\n\n__2. Cross Entropy:__  \n    Entropy is also an impurity measure and for a given node it can be calculated by\n    $$\n        Entropy(t) = - \\sum_{j = 1}^{C} P(j|t) * log_2(P(j|t)\n    $$\n    To measure the quality of a split we use information gain:\n    $$\n    GAIN_{split} = Entropy(Parent) - \\sum_{i = 1}^{k} \\frac{n_i}{n}Entropy(i)\n    $$\n    The split yielding highest Gain is prefered.\n\nThe process of splitting continues until a stopping criterion is reached. The stopping criteria plays a vital role to prevent overfitting on training data. Often pre-pruning is used to address the overfitting issue which includes stopping the algorithm before it becomes a fully grown tree. Among different stopping conditions, we choose tree depth and minimum impurity decrease as a hyperparameter for our model.  \nWe evaluate all the combinations of the following hyperparameters via K-fold cross validation:\n\n- criterion: {gini, entropy}\n- max depth: {6,8,10,12}\n- min impurity decrease: {0.0,0.001,0.0001}\n\nBest performing hyperparameters and classification results are as following:\n","metadata":{}},{"cell_type":"code","source":"models = {\n    \"DecisionTree\":DecisionTreeClassifier(random_state=rs),\n#     \"RandomForestClassifier\":RandomForestClassifier(random_state=rs),\n#     \"LogisticRegression\":LogisticRegression(random_state=rs),\n#     \"SVM\":SVC(random_state=rs)\n}\n\nparams = {\n        \"decisiontreeclassifier__min_impurity_decrease\": [0.0,0.001,0.0001],\n        \"decisiontreeclassifier__max_depth\": [6,7,8,10,12],\n        \"decisiontreeclassifier__criterion\":[\"gini\", \"entropy\"]\n    }\n\nfor name in models:\n    model = make_pipeline(StandardScaler(), SMOTE(random_state=rs), models[name])\n    run_tests(model, X_train, X_test, y_train, y_test, params, scores, gs_trained_models)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T01:06:04.945779Z","iopub.execute_input":"2021-07-18T01:06:04.946383Z","iopub.status.idle":"2021-07-18T01:06:06.323749Z","shell.execute_reply.started":"2021-07-18T01:06:04.946326Z","shell.execute_reply":"2021-07-18T01:06:06.322607Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"*********************************************************\nClassification report for DecisionTree on yet-unseen data\n*********************************************************\n\nAccuracy: 0.920\nf-score: 0.938\n\nBest Parameters:\n{\n    \"criterion\": \"gini\",\n    \"max_depth\": 6,\n    \"min_impurity_decrease\": 0.0\n} \n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x360 with 2 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":"\nThe strong point of decision trees is their explainability. They are inexpensive to construct and very easy to explain since a graphical representation of internal nodes and decisions is often available (at least for small trees). They can easily handle qualitative attributes without creating dummy variables.  \nThe major downside of decision trees is their tendency to overfit the training data. To overcome this aspect they are often used with bagging or boosting techniques to enhance their generalization ability.\n","metadata":{}},{"cell_type":"code","source":"clf = gs_trained_models[\"DecisionTree\"].named_steps['decisiontreeclassifier']\nclf.fit(X_train, y_train)\n\nfig, ax = plt.subplots(figsize=(20,14))\nout = tree.plot_tree(clf,\n               feature_names= X_train.columns,\n               class_names = [\"b\",\"g\"],\n               fontsize= 11, \n               filled = True, \n               label = 'all',\n               precision= 2,\n               rounded = True, \n#                ax = ax\n              )\nfor o in out:\n    arrow = o.arrow_patch\n    if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\n        arrow.set_arrowstyle(\"fancy\")\n        \nplt.figtext(0.5,0.07, \"Figure 9: Internal structure of trained tree model.\", fontsize = 20, ha = 'center')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T00:47:28.243695Z","iopub.execute_input":"2021-07-18T00:47:28.244083Z","iopub.status.idle":"2021-07-18T00:47:30.312351Z","shell.execute_reply.started":"2021-07-18T00:47:28.244053Z","shell.execute_reply":"2021-07-18T00:47:30.311254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 Random Forest\n\nRandom Forest is an ensemble of Decision Trees, generally trained via bagging methods. The bootstrap aggregation or bagging method combines the prediction bagging functions, often called weak learners, learned from multiple data sets, with a view to improving overall prediction accuracy. Ideally bagging requires obtaining multiple independent training sets from the population but it is not always possible. We use the so-called bootstrap data sets by sampling with replacement from the original training set. Bootstrap datasets have the same cardinality as the original dataset.  \n\nBagging is especially beneficial when dealing with predictors that tend to overfit the data, such as in decision trees, where the (unpruned) tree structure is very sensitive to small changes in the training set. However, in presence of a feature that provides a very good split of the data, the trees generated with bootstrapped datasets will have the same structure of initial nodes. Consequently we will end up with highly correlated predictors.  \n\nRandom forests differ from bagging techniques as they use bootstrap data sets in combination with feature bagging. In feature bagging, to construct each tree, we randomly select a subset of features (typically $\\log(p)$ or $\\sqrt{p}$) and use only these features to find the best split along the way. Trees generated using this technique no longer make the same splits hence we overcome the correlation problem caused by bagging.  \n\n<figure>\n  <center>\n      <img src=\"https://i.imgur.com/x06pwJb.png\" alt=\"Trulli\" style=\"width:80%; height:60%\">\n  </center>\n</figure>\n\nFor classification, the target label is calculated by taking a majority vote from all trained trees. In addition to hyperparameters introduced in section ___ for decision trees, we performed tuning for the number of trees to construct and the subset feature size of training data used to train.  We report the classification results and optimal hyperparameters settings found by K-fold cross validation in the following section.  \n\nAdditional hyperparameters:\n- n estimators: {80, 100, 120}\n- max features :{sqrt, log2}\n","metadata":{}},{"cell_type":"code","source":"models = {\n#     \"DecisionTree\":DecisionTreeClassifier(random_state=rs),\n    \"RandomForest\":RandomForestClassifier(random_state=rs),\n#     \"LogisticRegression\":LogisticRegression(random_state=rs),\n#     \"SVM\":SVC(random_state=rs)\n}\n\nparams = {\n        \"randomforestclassifier__n_estimators\":[80,100,120],\n        \"randomforestclassifier__criterion\":[\"gini\", \"entropy\"],\n        \"randomforestclassifier__max_depth\":[8,10,12],\n        \"randomforestclassifier__min_samples_split\":[2,4,6],\n        \"randomforestclassifier__max_features\":['sqrt','log2'],\n    }\n\nfor name in models:\n    model = make_pipeline(StandardScaler(),SMOTE(random_state=rs), models[name])\n    run_tests(model,X_train, X_test, y_train, y_test, params, scores, gs_trained_models)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T00:45:09.910557Z","iopub.status.idle":"2021-07-18T00:45:09.91103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nAs shown by the confusion matrix above, we have decreased the number of misclassified samples from 11 to 5 and the F1 score improved by 5.6 points. Random forests are more robust to noise and outliers so they are stable learners as compared with decision trees.  \nWhile the advantage of Random Forest in the sense of enhanced accuracy and f1-score is clear, we should\nalso consider its negative aspects and, in particular, the loss of interpretability. Specifically a random forest consists of many trees, thus making the prediction process both hard to visualize and interpret. It is hard to visualize and understand the prediction process based on 200 trees. However, Random Forest can provide some useful information about the input features and their importance in decision making. Each internal node of a decision tree induces a certain decrease in the training loss; we\ncan define the feature importance for feature $x_j$ in tree $T$ as, \n$$\n\\textbf{IMP}_{T}(x_j) = \\sum_{v\\ \\in\\ {T}} \\nabla_{loss}(v) \\mathbb{1}(x_j\\ is\\ associated\\ with\\ v)\n$$\nwhere $v$ is a decision node of $T$. To extend this idea within the random forest, the feature importance is averaged over all trees of the forest. Figure ___ shows the feature importance for different features used in random forest generation.\n","metadata":{}},{"cell_type":"code","source":"num_cols = X_train.select_dtypes(include= ['Float64', 'int64']).columns\nclf = gs_trained_models[\"RandomForest\"].named_steps['randomforestclassifier']\n\nplt.figure(figsize = (14,6))\nheight = clf.feature_importances_\norder = np.argsort(height)\nplt.bar(x = num_cols[order][::-1], height= height[order][::-1])\n\n# plt.title(\"Importance level of features used in Random Forest\")\nplt.xlabel(\"Feature\", size = 14)\nplt.ylabel(\"Importance level\", size = 14)\nplt.figtext(0.5,-0.019, \"Figure 10: Importance level of features used in Random Forest\", fontsize = 14, ha = 'center')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T00:45:09.912025Z","iopub.status.idle":"2021-07-18T00:45:09.91252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3 Logistic Regression\n\nLogistic Regression is a statistical learning technique. It is one of the simplest and most used Supervised Machine Learning algorithms for binary classification. It estimates the probability that an instance belongs to a particular class. If the estimated probability is greater than 50%, then the model predicts that the instance belongs to that class, or else it predicts that it belongs to the negative class.  \n\nIn logistic regression response variable $Y$ is usually interpreted as the probability of an event happening (y = 1) or not happening $(Y = 0)$. $P$ is the probability of that event happening ($Y = 1$) and $1-P$ is the probability of that event not happening $(Y = 0)$. If we consider all instances of response variables in our training data $T$ as IID, then P can be estimated using MLE: \n\n$$\n    MLE =  \\prod_{i \\in T}^{} P_i^{Y_i} (1-P_i)^{1 - {Y_i}} \\\\ \n$$\nby taking log of both sides,\n\n$$\n    \\log_{MLE} = \\sum_{i \\in T} Y_i\\log(P_i) + (1-Y_i)\\log(1-P_i) \\tag{1}\n$$\n\nRecall that the log odds ratio (or _logit_) for an event happening is defined as: $\\log\\big(P/(1-P)\\big)$\nThe logit is linear in the predictors so it can be written as a linear function of predictors X (similar to linear regression).\n$$\n    logit = \\log\\big(\\frac{P}{1-P}\\big) = W^T\\cdot X + b \\\\\n$$\nHence, \n$$\n    P = \\frac{1}{1 + e^{-W^T\\cdot X + b}} \\tag{2}\n$$\n\nby substituting value of P in eq 1,\n\n$$\n    \\log_{MLE} = \\sum_{i \\in T} Y_i\\log\\big(\\frac{1}{1 + e^{-W^T\\cdot X_i + b}}\\big) + (1-Y_i)\\log\\big(1-\\frac{1}{1 + e^{-W^T\\cdot X_i + b}}\\big)\n$$\nwhich simplifies to,\n\n$$\n    \\log_{MLE} = \\sum_{i \\in T} Y_i (W^T\\cdot X_i + b)) + (1-Y_i)\\log\\big(\\frac{1}{1+e^{W^T\\cdot X_i + b}}\\big)\n$$\nDifferent optimization techniques (like gradient descent) are used to find the solution for $\\hat{w} = argmax_w \\log_{MLE}$\nThe function describing probability in eq 2 is famously known as _sigmoid fucntion_ and we plot it in figure___.\n\n\n<figure>\n  <center>\n      <img src=\"https://i.imgur.com/DxaKSBb.png\" alt=\"Drawing\"/>  \n  </center>\n</figure>\n\n    \nThe Sigmoid function takes the linear combination of predictors and maps it into a [0,1] interval. Once the probabilities are obtained, the classification is made by applying a threshold to them, for example is assigned the label 0 if $P â‰¥ 0.5$.  \nTo avoid overfitting, different kinds of regularization methods are used to reduce the complexity of the model. This operation is performed by restoring the size of coefficients in vector $W$. We used l1 and l2 norms as regularization techniques in hyperparameters tuning. The former assigns zero values to needless weight coefficients and the later reduces their size to become less than 1. To control the effect of regularization we tuned another parameter, C. It represents the inverse of regularization. _Tol_ is the stopping criteria for learning algorithms.  \nWe tested all possible combinations of following hyperparameters via K-fold cross validation and detailed results are reported below.\n\n- penalty: {'l1' ,'l2' }\n- tol: {1e-2, 1e-3, 1e-4}\n- C: {0.5 ,0.8, 1.0, 1.2, 1.5} \n","metadata":{}},{"cell_type":"code","source":"models = {\n#     \"DecisionTree\":DecisionTreeClassifier(random_state=rs),\n#     \"RandomForest\":RandomForestClassifier(random_state=rs),\n    \"LogisticRegression\":LogisticRegression(random_state=rs),\n#     \"SVM\":SVC(random_state=rs)\n}\n\nparams = {\n        \"logisticregression__penalty\": ['l1','l2'],\n        \"logisticregression__tol\": [1e-2, 1e-3, 1e-4],\n        \"logisticregression__C\": [0.5,0.8, 1.0,1.2, 1.5] \n    }\n\nfor name in models:\n    model = make_pipeline(StandardScaler(),SMOTE(random_state=rs), models[name])\n    run_tests(model,X_train, X_test, y_train, y_test, params, scores, gs_trained_models)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T00:45:09.913565Z","iopub.status.idle":"2021-07-18T00:45:09.914072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nLogistic regression provides statistical interpretation of the results. Since it calculates the probability of an instance belonging to a certain class, we can set the threshold for hard label classification according to the problem at hand. A probability close to 0 or 1 shows that the model is very confident about its prediction whereas predictions which are closer to threshold value can be further evaluated with domain knowledge, without blindly accepting the results of the classifier.  \nSince we have all continuous numerical predictors in our dataset, we can easily understand which features correspond to Good returns and which one contributes mostly for Bad returns. The predictors which have a coefficient value of near zero, are the ones which contain no useful information for the learning process.  \n","metadata":{}},{"cell_type":"code","source":"cls = gs_trained_models['LogisticRegression'].named_steps['logisticregression']\nplt.figure(figsize = (16,6))\n\ncoeffs = cls.coef_.ravel()\nmask = coeffs > 0\ncolor = np.ones_like(mask, dtype='object')\ncolor[mask] = 'b'\ncolor[~mask] = 'r'\n\nplt.bar(X_train.columns, coeffs, color = color)\nplt.ylabel(\"coefficient weight\", size = 12)\nplt.xlabel(\"predictor\", size = 12)\nplt.grid()\nplt.figtext(0.5,-0.050, \"Figure 12: Coefficients overview of trained linear regression model.\", fontsize = 16, ha = 'center')\nplt.grid()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T00:45:09.915343Z","iopub.status.idle":"2021-07-18T00:45:09.915817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.4 SVM\n\nSVM is a supervised method that finds the hyperplane which maximizes the margin between two classes in the feature space. It comes in different flavors which we discuss in this section one by one.\n\n### 5.4.1 Hard-SVM\nA hard-SVM assumes that the data is linearly separable and seeks for the separating hyperplane with the largest margin. This margin ensures lower generalization error on yet-unseen data. The margin is defined as the minimum distance between points in our data and the hyperplane. (Figure __)\n\n<figure>\n  <center>\n  <img src=\"https://i.imgur.com/xpxxOVm.png\" alt=\"Drawing\" />\n</center>\n</figure>\nA separating hyperplane is defined by $w, b$ such that $\\forall i, y_i( w \\cdot x_i + b) > 0$ and the margin equals $min\\ | w\\cdot x_i + b|$ (assuming w is unitary). The closest points to hyperplane are called _Support Vectors._  \nThe problem of Hard-SVM can be defined as:\n\n$$\n\\underset{(w,b):\\ ||w|| = 1}{\\operatorname{argmax}} \\underset{i \\in T}{\\operatorname{min}} | w\\cdot X_i + b | \\\\\ns.t \\ \\forall i, y_i(w\\cdot x_i + b) > 0\n$$\n\nWhich can be defined in its dual form using Lagrange multipliers, \n\n$$\n\\underset{\\alpha}{\\operatorname{argmax}} \\sum_{i}^{m} \\alpha_i - \\frac{1}{2} \\sum_i^{m} \\sum_j^{m} \\alpha_i \\alpha_j y_iy_j (x_i\\cdot x_j) \\tag{3} \\\\\ns.t \\ \\forall i,\\ \\alpha_i > 0\\ and\\ \\sum_i^m \\alpha_iy_i = 0 \\\\\nand,\\ \\ w = \\sum_i^m \\alpha_iy_ix_i \n$$\n\nThe weight vector $w$ is a linear combination of training instances. A key to SVM's success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the correct side do not modify the fit; their position and number do not matter so long as they do not cross the margin. In eq 3, only the points on margin (support vectors) have $\\alpha > 0$, all other points have $\\alpha = 0$.\n\n### 5.4.2 Soft-margin SVM\n\nHard Margin SVM can only find a solution when data is perfectly separable by a hyperplane but It is not always possible to ensure. It may be rare to find that the data are linearly separable. When this happens, there may be many points within the margin or completely on the wrong side of the hyperplane.(figure __) In such cases, the best hyperplane is the one that has a minimum number of such points within the margin.  \nA soft-margin SVM classifier allows a certain number of data samples to violate the margin boundaries and a penalty is charged for every â€œcontaminantâ€ inside the margin or misclassified point. a soft-margin SVM problem can be defined as,  \n\n$$\n\\underset{(w,b):\\ ||w|| = 1}{\\operatorname{argmin}} \\frac{1}{2} {||w||}^2 + C \\sum_i^m \\xi_i \\\\\ns.t \\ \\forall i, y_i(w\\cdot x_i + b) > 1 - \\xi_i\\ \\text{and}\\ \\xi_i \\geq 0\n$$\n\n$\\xi_i$ are called the slack variables and they represent the penalty that is applied for each violation. The sum of all such errors is minimized to get the best separation.  Whereas, _C_ is the regularization term which trades off correct classification of training examples against maximization of the decision functionâ€™s margin. For larger values of C, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. A lower C will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy.  \nSimilar to Hard-margin classifier, we can define soft-margin problem in it's dual form using Lagrangian multipliers,\n\n$$\nL(w,b,\\alpha) = \\frac{1}{2}(w\\cdot w) + C\\sum_i^m \\xi_i - \\sum_i^m \\alpha_i[y_i(w\\cdot x + b) + \\xi_i -1] - \\sum_i^m \\eta_i\\xi_i\n$$\n  \nby taking partial derivatives and performing few simplifications, it gives us this nice form solution:  \n\n$$\n\\underset{\\alpha}{\\operatorname{argmax}} \\sum_{i}^{m} \\alpha_i - \\frac{1}{2} \\sum_i^{m} \\sum_j^{m} \\alpha_i \\alpha_j y_iy_j (x_i\\cdot x_j) \\tag{4} \\\\\ns.t \\ \\forall i,\\ \\alpha_i \\in [0, C] \\ and\\ \\sum_i^m \\alpha_iy_i = 0 \\\\\nand,\\ \\ w = \\sum_i^m \\alpha_iy_ix_i \n$$\n\nIt is worth noticing that the slack variables only affect the values $\\alpha_i$ can assume. Regularization parameter C is an upper bound on the values of $\\alpha_i$. \n\n\n### 5.4.3 Kernel SVM\nSo far we have talked about the cases when underline distribution of data allows us to separate them with a hyperplane in the input feature domain.  Soft margin SVM is able to handle outliers and some noise in given data, but if the classes are distributed in non linear shapes (Figure ), then we need non linear support vectors to define the margin and decision boundary.  \n\n<figure>\n  <center>\n        <img src=\"https://i.imgur.com/i38RSo4.png\" alt=\"Drawing\" />\n  </center>\n</figure>\n\n\nTo handle such data one possible solution is to map our data from input space to a feature space (figure ), fit nonlinear relationships with a linear classifier in feature space and finally map the decision boundaries back to the input space where data was not linearly separable (figure ). However, the mapping procedure has its own challenges as often it requires prior knowledge and mapping can be computationally expensive.  \n\nLet's take a step back and observe the dual form solutions described in eq __ and __ . The solution requires us to perform on the dot products between instances $x_i \\ and \\ x_j$ for $i,j \\in m$. What if we can define a function which takes as input our training samples in their input space and provide us their inner product in feature space?. This method is famously known as the _Kernel Trick_. A kernel is a function for a mapping $\\varphi$ which implements the inner product in feature space:\n\n$$\nK(x_i, x_j) = \\varphi(x_i)\\cdot \\varphi(x_j)\n$$\n\nwithout ever building the full representation of the kernel projection $\\varphi$. This kernel trick is one of the reasons the SVM method is so powerful.  \nThe kernel SVM problem and solution in its dual form is given by:\n\n$$\n\\underset{\\alpha}{\\operatorname{argmax}} \\sum_{i}^{m} \\alpha_i - \\frac{1}{2} \\sum_i^{m} \\sum_j^{m} \\alpha_i \\alpha_j y_iy_j (\\varphi(x_i)\\cdot \\varphi(x_j)) \\tag{5} \\\\\ns.t \\ \\forall i,\\ \\alpha_i \\in [0, C] \\ and\\ \\sum_i^m \\alpha_iy_i = 0 \\\\\nand,\\ \\ w = \\sum_i^m \\alpha_iy_i\\varphi(x_i)\n$$\n\n\n\nSome of the most popular kernels are rbf , polynomial and sigmoid.  \n- _K_ degree polynomial kernel is defined as:\n\n$$\nK(x_i, x_j)_{poly} = (1 + x_i\\cdot x_j)^k \\tag{6}\n$$  \n\n- RBF (radial basis function) kernel computes the distance using a gaussian curve and it is defined as:\n\n$$\nK(x_i, x_j)_{rbf} = e^{-\\frac{||x_i - x_j||^2}{2\\sigma}} \\tag{7}\n$$\n\nThe Gaussian kernel provides smaller values if the instances are far away from each other (in the original domain) and closer to 1 if they are close. Ïƒ is a parameter that controls the scale determining what we mean by â€œclose.â€  \n\nWe tested all possible combinations of following hyperparameters via K-fold cross validation and detailed results are reported below.\n\n- kernel: {'linear', 'rbf', 'poly'}\n- gamma: :{'scale', 'auto'}\n- C: {0.5,0.8, 1.0,1.2, 1.5} \n- tol: {1e-2,1e-3,1e-4}\n","metadata":{"execution":{"iopub.status.busy":"2021-07-17T19:39:21.027963Z","iopub.execute_input":"2021-07-17T19:39:21.028362Z","iopub.status.idle":"2021-07-17T19:39:21.070924Z","shell.execute_reply.started":"2021-07-17T19:39:21.028331Z","shell.execute_reply":"2021-07-17T19:39:21.069471Z"}}},{"cell_type":"code","source":"models = {\n#     \"DecisionTree\":DecisionTreeClassifier(random_state=rs),\n#     \"RandomForest\":RandomForestClassifier(random_state=rs),\n#     \"LogisticRegression\":LogisticRegression(random_state=rs),\n    \"SVM\":SVC(random_state=rs)\n}\n\nparams = {\n        \"svc__kernel\": ['linear', 'rbf', 'poly'],\n        \"svc__gamma\" :['scale', 'auto'],\n        \"svc__tol\": [1e-2,1e-3,1e-4],\n        \"svc__C\": [0.5,0.8, 1.0,1.2, 1.5] \n    }\n\nfor name in models:\n    model = make_pipeline(StandardScaler(),SMOTE(random_state=rs), models[name])\n    run_tests(model,X_train, X_test, y_train, y_test, params, scores, gs_trained_models)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T00:45:09.916823Z","iopub.status.idle":"2021-07-18T00:45:09.91741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Dimensionality reduction\n\nDimensionality reduction is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data.  There are several reasons to reduce the dimensionality of the data. First, high dimensional data impose computational challenges. Moreover, in some situations high dimensionality might lead to poor generalization abilities of the learning algorithm.   \nPrincipal Component Analysis (PCA) and Random projections are two examples of dimensionality reduction techniques. Both methods use feature projection techniques where these projections are linear transformations of a given dataset from high dimensional space to a lower dimensional space. We briefly explain the philosophy behind these approaches and select the one which suits our problem at hand.  \n\n## 6.1 Random Projections\n    \nThe idea behind Random projections is to map our data in a new, low dimensional space in such a way that preserves the distances between instances of data. We do not care about the reconstruction of our data back into original space.  \nPrecisely, it is the $R^d \\mapsto R^n$ transformation given by $x  \\longmapsto Wx $ s.t $ n\\ll d\\ \\text{and}\\ \\forall i, j \\in m\\ \\ \\frac{\\lVert Wx_i - Wx_j \\rVert}{\\lVert x_i, x_j \\lVert} \\approx 1$  \nInterestingly we can perform this transformation by using a random matrix W s.t $ W_{ij}  \\sim \\mathcal{N}(0,\\,1/n)\\ $ and required constraints are given by **Johnson-Lindenstrauss lemma:**\n<br>\n<br>\ngiven $\\ \\epsilon \\in (0,1)\\ $ the tolerance level, m points in our dataset and \n$$\nn \\geq \\frac{4 \\log m}{\\frac{\\epsilon^2}{2} - \\frac{\\epsilon^3}{3}} \\tag{i}\n$$\nthen there exist a linear mapping $f: R^d \\mapsto R^n$ s.t $\\forall i, j \\in m$ \n$$\n(1-\\epsilon) \\lVert x_i-x_j \\lVert \\ \\leq \\ \\lVert f(x_i) - f(x_j) \\lVert \\ \\leq \\ (1+\\epsilon) \\lVert x_i-x_j \\lVert\n$$\n<br>\n<br>\nIt means that if we fix $\\epsilon$ to 0.1 for our dataset which originally has 34 features (d) and 263 samples (m), by constructing a random square matrix of size 4776 x 4776 we are guaranteed to have a distance distortion bounded by the scale of ($1 \\pm \\epsilon $).  \n\nClearly it is counter productive for our data and we need to find some other technique for dimensionality reduction but notice that eq (i) is independent from the dimensionality of our original dataset. It depends only on the cardinality of training data and required tolerance level. If we had a RNA-Seq database of 1000 samples, which has over 20,000 attributes per sample, we can achieve $\\epsilon$ 0.1 by using just 5920 features, randomly transformed by linear mapping.\n\n","metadata":{}},{"cell_type":"markdown","source":"## 6.2 Principal Component Analysis (PCA)\n\nDifferent from Random projections, in PCA we fix the number of components (i.e dimensionality of lower space (n)) and find a mapping $x  \\longmapsto Wx $ which provides best recovery of x from $y = Wx$. We define this linear recovery with the help of two matrices $U, W$ s.t $\\tilde x = UWx$  \nThe matrix W $\\in \\mathbb{R}^{n,d}$ is known as an encoder matrix as it transforms the d x 1 vector into n x 1 vector. The matrix U $\\in \\mathbb{R}^{d,n}$ instead is known as a decoder matrix as it transforms back into original data space. PCA problem is solved by ERM defined as,  \n\n$$\n\\underset{W  \\in \\mathbb{R}^{n,d},  U \\in \\mathbb{R}^{d,n} }{\\operatorname{argmax}} \\\n\\sum_i^m \\lVert x_i - UWx_i \\lVert^2 \\tag{ii}\n$$\n\nBy assuming that $W = U'$ and the columns of U are orthonormal, we can simplify eq (ii) into,\n$$\n\\underset{U \\in \\mathbb{R}^{d,n}: U'U = \\textit{I}}{\\operatorname{argmax}} \ntrace\\big(U'\\big(\\sum_i^m x_ix_i'\\big)U\\big) \\tag{iii}\n$$\n\nIt is common practice to center the matrix X by subtracting the feature mean from its columns. $\\sum_i^m x_ix_i'$ is called Scatter Matrix and it can also be written as $A:=X'X$. Since A is symmetric, it has non negative eigenvalues with orthonormal eigenvectors. The solution to PCA problem is to set columns of U the n eigenvectors of scatter matrix $v_1,v_2,...,v_n$ corresponding to n largest eigenvalues $\\lambda_1 \\geq \\lambda_2,...,\\geq \\lambda_n$.  \n\nPCA can also be seen as a transformation of our data into a new coordinate system where greatest variance induced by linear projection of input data lies on the first coordinate. We call this coordinate the _first principal component_. Similarly, coordinate represents projection with largest variance and so on. We can define a principal component as linear combination of the columns of input data:\n\n$$\npc_j = w_1^{j}x_1 + w_2^{j}x_2 + .. + w_n^{j}x_n \\ \\ where\\ n\\leq d \\\\\ns.t \\ w_i \\ and \\ w_j \\ \\text{are ON}\\ \\forall \\ i,j \\leq n \\ and \\ i \\neq j\n$$\n\n\nThe variance of first principal component is given by:\n\n$$\nVAR(pc_1) = VAR( X w^{1} ) = (w^{1})' S w^{1}\n$$\n\nS is the Covariance matrix of X. The optimization problem for variance can be written as:\n\n$$\n\\underset{w^{1}: \\lVert w^{1} \\lVert = 1}{\\operatorname{argmax}} VAR( X w^{1} ) = \\underset{w^{1}: \\lVert w^{1} \\lVert = 1}{\\operatorname{argmax}} (w^{1})' S w^{1}\n$$\n\nUsing Langrangian multipliers:\n\n$$\nL(w, \\lambda) = w' S w - \\lambda (w'w-1)\n$$\n\nBy taking partial derivatives and setting them equal to 0 we get:\n\n$$\nw'w = 1 \\ and \\ Sw = \\lambda w \\\\\n\\Longrightarrow VAR( X w^{1} ) = (w^{1})' S w^{1} = (w^{1})' \\lambda w^{1} = \\lambda\n$$\n\nFew things to notice here:\n1. $\\lambda$ is the _amount_ of variance in $pc_1$\n2. $\\lambda$ is an eigenvalue of Covariance matrix $S$\n3. $pc_1$ can have maximum variance if we set the eigenvector corresponding to the $max\\{\\lambda_i\\}$ as $w$.\n\nSince the Covariance matrix is Symmetric, all its eigenvectors are ON. We can use $n$ of those eigenvectors corresponding to $n$ largent eigenvalues to build $U$ matrix.  It is important to normalize all input features before PCA as we don't want PCA to be biased towards features which have higher variance simply because of their scale (e.g. Home square meters vs n. of bathrooms).  \nAs stated before, the variance of i-th principal component is exactly equal to the associated eigenvalue $\\lambda_i$ and we can define __Ratio of explained variance__ as \n\n$$RVE_n = \\frac{\\lambda_1 + \\lambda_2 +..+ \\lambda_n}{\\lambda_1 + \\lambda_2 +..+ \\lambda_d} = \\frac{\\sum_i^nVAR(pc_i)}{VAR(X)}\n$$\n\nIf figure __ we show the scree plot along with cumulative explained variance. We set a threshold of 90% for explained variance ratio which yields $n = 18$ to be used as the number of components for PCA. \nTo summarize, we can represent our original data (34-dim) by PCA linear transformation into 18-dim space and it would cost us 10% of the explained variance.\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components= X_train_all.shape[1], random_state=rs)\nX_train_tr = pca.fit_transform(X_train_all)\n\nplt.figure(figsize = (12,8))\nplt.bar(range(1,len(pca.explained_variance_ratio_ )+1),\n         pca.explained_variance_ratio_, \n        )\nplt.plot(range(1,len(pca.explained_variance_ratio_ )+1),\n         pca.explained_variance_ratio_, \n         c = 'black',\n         linewidth = 0.7\n        )\nplt.plot(range(1,len(pca.explained_variance_ratio_ )+1),\n         np.cumsum(pca.explained_variance_ratio_),\n         c='red',\n         label=\"Cumulative Explained Variance ratio\",\n         linewidth = 0.8\n        )\n\n\nplt.ylabel('Explained variance ratio',fontdict= {\"size\":16})\nplt.xlabel('Components',fontdict= {\"size\":16})\nplt.legend(loc='upper left')\nplt.figtext(0.5,-0.019, \"Figure 15: PCA Scree plot and cumulative explained variance ratio\", fontsize = 16, ha = 'center')\nplt.show()\n\ncs = np.cumsum(pca.explained_variance_ratio_)\nthreshold = .90\nmask = cs < threshold\nn = sum(mask) + 1\npca_scores = {}\npca_gs_trained_models = {}","metadata":{"execution":{"iopub.status.busy":"2021-07-18T00:45:09.91869Z","iopub.status.idle":"2021-07-18T00:45:09.919135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"4.5\">\n\nWe replaced manual feature selection from our initial pipeline with PCA transformation Figure __ . The classification results for all learning methods introduced in section __ are reported in table __ .\n\n<figure>\n  <center>\n      <img src=\"https://i.imgur.com/WaoC8Qr.png alt=\"Trulli\" style=\"width:50%;height:50%\">\n  </center>\n</figure>\n\n<figure>\n  <center>\n      <img src=\"https://i.imgur.com/Y965fPl.png\" alt=\"Trulli\" style=\"width:50%;height:50%\">\n  </center>\n</figure>","metadata":{}},{"cell_type":"markdown","source":"# 7. Conclusions\n<font size=\"4.5\">\nAs we can see from Table 1, all the models have a pretty satisfying accuracy and f1-score. All the models provide comparable results once we add PCA in our pipeline. \nLogisitc regression benefits the most from PCA as data has lower dimensionality and all the features are ancorrelated. Decision trees provide excellent explainability but they have stability issues which we can overcome by using Random Forests. Support vector machine provides the best results in both with and without PCA cases. Since we are using RBG kernel in SVM, it has the lowest explainability among all other methods. \n</font>","metadata":{}}]}